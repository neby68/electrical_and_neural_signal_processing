<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>MNE_tutorial_EEG_MEG_processing &mdash; Electrical and neural signal processing Example 0.0.0 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=7026087e"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Electrical and neural signal processing Example
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../modules.html">src</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Links:</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/neby68/electrical_and_neural_signal_processing">Git Repository</a></li>
<li class="toctree-l1"><a class="reference external" href="https://neby68.github.io/Signal_processing_toolbox/">Signal Processing toolbox's documentation</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Electrical and neural signal processing Example</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Module code</a></li>
      <li class="breadcrumb-item active">MNE_tutorial_EEG_MEG_processing</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for MNE_tutorial_EEG_MEG_processing</h1><div class="highlight"><pre>
<span></span>




<div class="viewcode-block" id="MNE_tutorial_EEG_MEG_processing">
<a class="viewcode-back" href="../MNE_tutorial_EEG_MEG_processing.html#MNE_tutorial_EEG_MEG_processing.MNE_tutorial_EEG_MEG_processing">[docs]</a>
<span class="k">def</span> <span class="nf">MNE_tutorial_EEG_MEG_processing</span><span class="p">():</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Neural data preprocessing and visualization using MNE library.</span>
<span class="sd">    ! It is a MNE tutorial!</span>

<span class="sd">    </span>
<span class="sd">    1. Plot the raw data</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">        .. image:: _static/images//MNE_tutorial/Raw_data.png</span>

<span class="sd">        </span>
<span class="sd">        .. image:: _static/images//MNE_tutorial/Raw_data_all_channel.png</span>

<span class="sd">        </span>
<span class="sd">    2. Perform ICA</span>


<span class="sd">        - Check for artifact</span>


<span class="sd">            The following figure show example of artifacts</span>


<span class="sd">            .. image:: _static/images//MNE_tutorial/ICA_1_and_2.png</span>

<span class="sd">        </span>
<span class="sd">        - Compare the raw data before and after removing artifact</span>


<span class="sd">            .. image:: _static/images//MNE_tutorial/raw_data_whithout_artifaact.png</span>

<span class="sd">        </span>
<span class="sd">    3. Compute EFR and ERP</span>


<span class="sd">        .. image:: _static/images//MNE_tutorial/ERF.png</span>


<span class="sd">        .. image:: _static/images//MNE_tutorial/ERP.png</span>


<span class="sd">    4. Analyse Time frequency response</span>


<span class="sd">        .. image:: _static/images//MNE_tutorial/time_frequency_plot.png</span>

<span class="sd">    </span>
<span class="sd">    5. Analyse Topography</span>


<span class="sd">        .. image:: _static/images/topography_mne_example.png</span>

<span class="sd">        </span>

<span class="sd">    &quot;&quot;&quot;</span>


    <span class="c1"># .. _tut-overview:</span>

    <span class="c1"># ============================================</span>
    <span class="c1"># Overview of MEG/EEG analysis with MNE-Python</span>
    <span class="c1"># ============================================</span>

    <span class="c1"># This tutorial covers the basic EEG/MEG pipeline for event-related analysis:</span>
    <span class="c1"># loading data, epoching, averaging, plotting, and estimating cortical activity</span>
    <span class="c1"># from sensor data. It introduces the core MNE-Python data structures</span>
    <span class="c1"># `~mne.io.Raw`, `~mne.Epochs`, `~mne.Evoked`, and `~mne.SourceEstimate`, and</span>
    <span class="c1"># covers a lot of ground fairly quickly (at the expense of depth). Subsequent</span>
    <span class="c1"># tutorials address each of these topics in greater detail.</span>

    <span class="c1"># We begin by importing the necessary Python modules:</span>

    <span class="c1"># %%</span>

    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
    <span class="kn">import</span> <span class="nn">mne</span>

    <span class="c1"># %%</span>
    <span class="c1"># Loading data</span>
    <span class="c1"># ^^^^^^^^^^^^</span>
    <span class="c1">#</span>
    <span class="c1"># MNE-Python data structures are based around the FIF file format from</span>
    <span class="c1"># Neuromag, but there are reader functions for :ref:`a wide variety of other</span>
    <span class="c1"># data formats &lt;data-formats&gt;`. MNE-Python also has interfaces to a</span>
    <span class="c1"># variety of :ref:`publicly available datasets &lt;datasets&gt;`, which MNE-Python</span>
    <span class="c1"># can download and manage for you.</span>
    <span class="c1">#</span>
    <span class="c1"># We&#39;ll start this tutorial by loading one of the example datasets (called</span>
    <span class="c1"># &quot;:ref:`sample-dataset`&quot;), which contains EEG and MEG data from one subject</span>
    <span class="c1"># performing an audiovisual experiment, along with structural MRI scans for</span>
    <span class="c1"># that subject. The `mne.datasets.sample.data_path` function will automatically</span>
    <span class="c1"># download the dataset if it isn&#39;t found in one of the expected locations, then</span>
    <span class="c1"># return the directory path to the dataset (see the documentation of</span>
    <span class="c1"># `~mne.datasets.sample.data_path` for a list of places it checks before</span>
    <span class="c1"># downloading). Note also that for this tutorial to run smoothly on our</span>
    <span class="c1"># servers, we&#39;re using a filtered and downsampled version of the data</span>
    <span class="c1"># (:file:`sample_audvis_filt-0-40_raw.fif`), but an unfiltered version</span>
    <span class="c1"># (:file:`sample_audvis_raw.fif`) is also included in the sample dataset and</span>
    <span class="c1"># could be substituted here when running the tutorial locally.</span>

    <span class="n">sample_data_folder</span> <span class="o">=</span> <span class="n">mne</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">sample</span><span class="o">.</span><span class="n">data_path</span><span class="p">()</span>
    <span class="n">sample_data_raw_file</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sample_data_folder</span> <span class="o">/</span> <span class="s2">&quot;MEG&quot;</span> <span class="o">/</span> <span class="s2">&quot;sample&quot;</span> <span class="o">/</span> <span class="s2">&quot;sample_audvis_filt-0-40_raw.fif&quot;</span>
    <span class="p">)</span>
    <span class="n">raw</span> <span class="o">=</span> <span class="n">mne</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">read_raw_fif</span><span class="p">(</span><span class="n">sample_data_raw_file</span><span class="p">)</span>

    <span class="c1"># %%</span>
    <span class="c1"># By default, `~mne.io.read_raw_fif` displays some information about the file</span>
    <span class="c1"># it&#39;s loading; for example, here it tells us that there are four &quot;projection</span>
    <span class="c1"># items&quot; in the file along with the recorded data; those are :term:`SSP</span>
    <span class="c1"># projectors &lt;projector&gt;` calculated to remove environmental noise from the MEG</span>
    <span class="c1"># signals, plus a projector to mean-reference the EEG channels; these are</span>
    <span class="c1"># discussed in the tutorial :ref:`tut-projectors-background`. In addition to</span>
    <span class="c1"># the information displayed during loading, you can get a glimpse of the basic</span>
    <span class="c1"># details of a `~mne.io.Raw` object by printing it; even more is available by</span>
    <span class="c1"># printing its ``info`` attribute (a `dictionary-like object &lt;mne.Info&gt;` that</span>
    <span class="c1"># is preserved across `~mne.io.Raw`, `~mne.Epochs`, and `~mne.Evoked` objects).</span>
    <span class="c1"># The ``info`` data structure keeps track of channel locations, applied</span>
    <span class="c1"># filters, projectors, etc. Notice especially the ``chs`` entry, showing that</span>
    <span class="c1"># MNE-Python detects different sensor types and handles each appropriately. See</span>
    <span class="c1"># :ref:`tut-info-class` for more on the `~mne.Info` class.</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">raw</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">raw</span><span class="o">.</span><span class="n">info</span><span class="p">)</span>

    <span class="c1"># %%</span>
    <span class="c1"># `~mne.io.Raw` objects also have several built-in plotting methods; here we</span>
    <span class="c1"># show the power spectral density (PSD) for each sensor type with</span>
    <span class="c1"># `~mne.io.Raw.compute_psd`, as well as a plot of the raw sensor traces with</span>
    <span class="c1"># `~mne.io.Raw.plot`. In the PSD plot, we&#39;ll only plot frequencies below 50 Hz</span>
    <span class="c1"># (since our data are low-pass filtered at 40 Hz). In interactive Python</span>
    <span class="c1"># sessions, `~mne.io.Raw.plot` is interactive and allows scrolling, scaling,</span>
    <span class="c1"># bad channel marking, annotations, projector toggling, etc.</span>

    <span class="n">raw</span><span class="o">.</span><span class="n">compute_psd</span><span class="p">(</span><span class="n">fmax</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">picks</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">exclude</span><span class="o">=</span><span class="s2">&quot;bads&quot;</span><span class="p">)</span>
    <span class="n">raw</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">duration</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_channels</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

    <span class="c1"># %%</span>
    <span class="c1"># Preprocessing</span>
    <span class="c1"># ^^^^^^^^^^^^^</span>
    <span class="c1">#</span>
    <span class="c1"># MNE-Python supports a variety of preprocessing approaches and techniques</span>
    <span class="c1"># (maxwell filtering, signal-space projection, independent components analysis,</span>
    <span class="c1"># filtering, downsampling, etc); see the full list of capabilities in the</span>
    <span class="c1"># :mod:`mne.preprocessing` and :mod:`mne.filter` submodules. Here we&#39;ll clean</span>
    <span class="c1"># up our data by performing independent components analysis</span>
    <span class="c1"># (`~mne.preprocessing.ICA`); for brevity we&#39;ll skip the steps that helped us</span>
    <span class="c1"># determined which components best capture the artifacts (see</span>
    <span class="c1"># :ref:`tut-artifact-ica` for a detailed walk-through of that process).</span>

    <span class="c1"># set up and fit the ICA</span>
    <span class="n">ica</span> <span class="o">=</span> <span class="n">mne</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">ICA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">97</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">800</span><span class="p">)</span>
    <span class="n">ica</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">raw</span><span class="p">)</span>
    <span class="n">ica</span><span class="o">.</span><span class="n">exclude</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>  <span class="c1"># details on how we picked these are omitted here</span>
    <span class="n">ica</span><span class="o">.</span><span class="n">plot_properties</span><span class="p">(</span><span class="n">raw</span><span class="p">,</span> <span class="n">picks</span><span class="o">=</span><span class="n">ica</span><span class="o">.</span><span class="n">exclude</span><span class="p">)</span>

    <span class="c1"># %%</span>
    <span class="c1"># Once we&#39;re confident about which component(s) we want to remove, we pass them</span>
    <span class="c1"># as the ``exclude`` parameter and then apply the ICA to the raw signal. The</span>
    <span class="c1"># `~mne.preprocessing.ICA.apply` method requires the raw data to be loaded into</span>
    <span class="c1"># memory (by default it&#39;s only read from disk as-needed), so we&#39;ll use</span>
    <span class="c1"># `~mne.io.Raw.load_data` first. We&#39;ll also make a copy of the `~mne.io.Raw`</span>
    <span class="c1"># object so we can compare the signal before and after artifact removal</span>
    <span class="c1"># side-by-side:</span>

    <span class="n">orig_raw</span> <span class="o">=</span> <span class="n">raw</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">raw</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
    <span class="n">ica</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">raw</span><span class="p">)</span>

    <span class="c1"># show some frontal channels to clearly illustrate the artifact removal</span>
    <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;MEG 0111&quot;</span><span class="p">,</span>
        <span class="s2">&quot;MEG 0121&quot;</span><span class="p">,</span>
        <span class="s2">&quot;MEG 0131&quot;</span><span class="p">,</span>
        <span class="s2">&quot;MEG 0211&quot;</span><span class="p">,</span>
        <span class="s2">&quot;MEG 0221&quot;</span><span class="p">,</span>
        <span class="s2">&quot;MEG 0231&quot;</span><span class="p">,</span>
        <span class="s2">&quot;MEG 0311&quot;</span><span class="p">,</span>
        <span class="s2">&quot;MEG 0321&quot;</span><span class="p">,</span>
        <span class="s2">&quot;MEG 0331&quot;</span><span class="p">,</span>
        <span class="s2">&quot;MEG 1511&quot;</span><span class="p">,</span>
        <span class="s2">&quot;MEG 1521&quot;</span><span class="p">,</span>
        <span class="s2">&quot;MEG 1531&quot;</span><span class="p">,</span>
        <span class="s2">&quot;EEG 001&quot;</span><span class="p">,</span>
        <span class="s2">&quot;EEG 002&quot;</span><span class="p">,</span>
        <span class="s2">&quot;EEG 003&quot;</span><span class="p">,</span>
        <span class="s2">&quot;EEG 004&quot;</span><span class="p">,</span>
        <span class="s2">&quot;EEG 005&quot;</span><span class="p">,</span>
        <span class="s2">&quot;EEG 006&quot;</span><span class="p">,</span>
        <span class="s2">&quot;EEG 007&quot;</span><span class="p">,</span>
        <span class="s2">&quot;EEG 008&quot;</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="n">chan_idxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">raw</span><span class="o">.</span><span class="n">ch_names</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">ch</span><span class="p">)</span> <span class="k">for</span> <span class="n">ch</span> <span class="ow">in</span> <span class="n">chs</span><span class="p">]</span>
    <span class="n">orig_raw</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="n">chan_idxs</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">duration</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">raw</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="n">chan_idxs</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">duration</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

    <span class="c1"># %%</span>
    <span class="c1"># .. _overview-tut-events-section:</span>
    <span class="c1">#</span>
    <span class="c1"># Detecting experimental events</span>
    <span class="c1"># ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</span>
    <span class="c1">#</span>
    <span class="c1"># The sample dataset includes several :term:`&quot;STIM&quot; channels &lt;stim channel&gt;`</span>
    <span class="c1"># that recorded electrical signals sent from the stimulus delivery computer (as</span>
    <span class="c1"># brief DC shifts / squarewave pulses). These pulses (often called &quot;triggers&quot;)</span>
    <span class="c1"># are used in this dataset to mark experimental events: stimulus onset,</span>
    <span class="c1"># stimulus type, and participant response (button press). The individual STIM</span>
    <span class="c1"># channels are combined onto a single channel, in such a way that voltage</span>
    <span class="c1"># levels on that channel can be unambiguously decoded as a particular event</span>
    <span class="c1"># type. On older Neuromag systems (such as that used to record the sample data)</span>
    <span class="c1"># this summation channel was called ``STI 014``, so we can pass that channel</span>
    <span class="c1"># name to the `mne.find_events` function to recover the timing and identity of</span>
    <span class="c1"># the stimulus events.</span>

    <span class="n">events</span> <span class="o">=</span> <span class="n">mne</span><span class="o">.</span><span class="n">find_events</span><span class="p">(</span><span class="n">raw</span><span class="p">,</span> <span class="n">stim_channel</span><span class="o">=</span><span class="s2">&quot;STI 014&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">events</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>  <span class="c1"># show the first 5</span>

    <span class="c1"># %%</span>
    <span class="c1"># The resulting events array is an ordinary 3-column :class:`NumPy array</span>
    <span class="c1"># &lt;numpy.ndarray&gt;`, with sample number in the first column and integer event ID</span>
    <span class="c1"># in the last column; the middle column is usually ignored. Rather than keeping</span>
    <span class="c1"># track of integer event IDs, we can provide an *event dictionary* that maps</span>
    <span class="c1"># the integer IDs to experimental conditions or events. In this dataset, the</span>
    <span class="c1"># mapping looks like this:</span>
    <span class="c1">#</span>
    <span class="c1"># .. _sample-data-event-dict-table:</span>
    <span class="c1">#</span>
    <span class="c1"># +----------+----------------------------------------------------------+</span>
    <span class="c1"># | Event ID | Condition                                                |</span>
    <span class="c1"># +==========+==========================================================+</span>
    <span class="c1"># | 1        | auditory stimulus (tone) to the left ear                 |</span>
    <span class="c1"># +----------+----------------------------------------------------------+</span>
    <span class="c1"># | 2        | auditory stimulus (tone) to the right ear                |</span>
    <span class="c1"># +----------+----------------------------------------------------------+</span>
    <span class="c1"># | 3        | visual stimulus (checkerboard) to the left visual field  |</span>
    <span class="c1"># +----------+----------------------------------------------------------+</span>
    <span class="c1"># | 4        | visual stimulus (checkerboard) to the right visual field |</span>
    <span class="c1"># +----------+----------------------------------------------------------+</span>
    <span class="c1"># | 5        | smiley face (catch trial)                                |</span>
    <span class="c1"># +----------+----------------------------------------------------------+</span>
    <span class="c1"># | 32       | subject button press                                     |</span>
    <span class="c1"># +----------+----------------------------------------------------------+</span>

    <span class="n">event_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;auditory/left&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;auditory/right&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="s2">&quot;visual/left&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s2">&quot;visual/right&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
        <span class="s2">&quot;smiley&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
        <span class="s2">&quot;buttonpress&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="c1"># %%</span>
    <span class="c1"># Event dictionaries like this one are used when extracting epochs from</span>
    <span class="c1"># continuous data; the ``/`` character in the dictionary keys allows pooling</span>
    <span class="c1"># across conditions by requesting partial condition descriptors (i.e.,</span>
    <span class="c1"># requesting ``&#39;auditory&#39;`` will select all epochs with Event IDs 1 and 2;</span>
    <span class="c1"># requesting ``&#39;left&#39;`` will select all epochs with Event IDs 1 and 3). An</span>
    <span class="c1"># example of this is shown in the next section. There is also a convenient</span>
    <span class="c1"># `~mne.viz.plot_events` function for visualizing the distribution of events</span>
    <span class="c1"># across the duration of the recording (to make sure event detection worked as</span>
    <span class="c1"># expected). Here we&#39;ll also make use of the `~mne.Info` attribute to get the</span>
    <span class="c1"># sampling frequency of the recording (so our x-axis will be in seconds instead</span>
    <span class="c1"># of in samples).</span>

    <span class="n">fig</span> <span class="o">=</span> <span class="n">mne</span><span class="o">.</span><span class="n">viz</span><span class="o">.</span><span class="n">plot_events</span><span class="p">(</span>
        <span class="n">events</span><span class="p">,</span> <span class="n">event_id</span><span class="o">=</span><span class="n">event_dict</span><span class="p">,</span> <span class="n">sfreq</span><span class="o">=</span><span class="n">raw</span><span class="o">.</span><span class="n">info</span><span class="p">[</span><span class="s2">&quot;sfreq&quot;</span><span class="p">],</span> <span class="n">first_samp</span><span class="o">=</span><span class="n">raw</span><span class="o">.</span><span class="n">first_samp</span>
    <span class="p">)</span>

    <span class="c1"># %%</span>
    <span class="c1"># For paradigms that are not event-related (e.g., analysis of resting-state</span>
    <span class="c1"># data), you can extract regularly spaced (possibly overlapping) spans of data</span>
    <span class="c1"># by creating events using `mne.make_fixed_length_events` and then proceeding</span>
    <span class="c1"># with epoching as described in the next section.</span>
    <span class="c1">#</span>
    <span class="c1">#</span>
    <span class="c1"># .. _tut-section-overview-epoching:</span>
    <span class="c1">#</span>
    <span class="c1"># Epoching continuous data</span>
    <span class="c1"># ^^^^^^^^^^^^^^^^^^^^^^^^</span>
    <span class="c1">#</span>
    <span class="c1"># The `~mne.io.Raw` object and the events array are the bare minimum needed to</span>
    <span class="c1"># create an `~mne.Epochs` object, which we create with the `~mne.Epochs` class</span>
    <span class="c1"># constructor. Here we&#39;ll also specify some data quality constraints: we&#39;ll</span>
    <span class="c1"># reject any epoch where peak-to-peak signal amplitude is beyond reasonable</span>
    <span class="c1"># limits for that channel type. This is done with a *rejection dictionary*; you</span>
    <span class="c1"># may include or omit thresholds for any of the channel types present in your</span>
    <span class="c1"># data. The values given here are reasonable for this particular dataset, but</span>
    <span class="c1"># may need to be adapted for different hardware or recording conditions. For a</span>
    <span class="c1"># more automated approach, consider using the `autoreject package`_.</span>

    <span class="n">reject_criteria</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">mag</span><span class="o">=</span><span class="mf">4000e-15</span><span class="p">,</span>  <span class="c1"># 4000 fT</span>
        <span class="n">grad</span><span class="o">=</span><span class="mf">4000e-13</span><span class="p">,</span>  <span class="c1"># 4000 fT/cm</span>
        <span class="n">eeg</span><span class="o">=</span><span class="mf">150e-6</span><span class="p">,</span>  <span class="c1"># 150 µV</span>
        <span class="n">eog</span><span class="o">=</span><span class="mf">250e-6</span><span class="p">,</span>
    <span class="p">)</span>  <span class="c1"># 250 µV</span>

    <span class="c1"># %%</span>
    <span class="c1"># We&#39;ll also pass the event dictionary as the ``event_id`` parameter (so we can</span>
    <span class="c1"># work with easy-to-pool event labels instead of the integer event IDs), and</span>
    <span class="c1"># specify ``tmin`` and ``tmax`` (the time relative to each event at which to</span>
    <span class="c1"># start and end each epoch). As mentioned above, by default `~mne.io.Raw` and</span>
    <span class="c1"># `~mne.Epochs` data aren&#39;t loaded into memory (they&#39;re accessed from disk only</span>
    <span class="c1"># when needed), but here we&#39;ll force loading into memory using the</span>
    <span class="c1"># ``preload=True`` parameter so that we can see the results of the rejection</span>
    <span class="c1"># criteria being applied:</span>

    <span class="n">epochs</span> <span class="o">=</span> <span class="n">mne</span><span class="o">.</span><span class="n">Epochs</span><span class="p">(</span>
        <span class="n">raw</span><span class="p">,</span>
        <span class="n">events</span><span class="p">,</span>
        <span class="n">event_id</span><span class="o">=</span><span class="n">event_dict</span><span class="p">,</span>
        <span class="n">tmin</span><span class="o">=-</span><span class="mf">0.2</span><span class="p">,</span>
        <span class="n">tmax</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">reject</span><span class="o">=</span><span class="n">reject_criteria</span><span class="p">,</span>
        <span class="n">preload</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># %%</span>
    <span class="c1"># Next we&#39;ll pool across left/right stimulus presentations so we can compare</span>
    <span class="c1"># auditory versus visual responses. To avoid biasing our signals to the left or</span>
    <span class="c1"># right, we&#39;ll use `~mne.Epochs.equalize_event_counts` first to randomly sample</span>
    <span class="c1"># epochs from each condition to match the number of epochs present in the</span>
    <span class="c1"># condition with the fewest good epochs.</span>

    <span class="n">conds_we_care_about</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;auditory/left&quot;</span><span class="p">,</span> <span class="s2">&quot;auditory/right&quot;</span><span class="p">,</span> <span class="s2">&quot;visual/left&quot;</span><span class="p">,</span> <span class="s2">&quot;visual/right&quot;</span><span class="p">]</span>
    <span class="n">epochs</span><span class="o">.</span><span class="n">equalize_event_counts</span><span class="p">(</span><span class="n">conds_we_care_about</span><span class="p">)</span>  <span class="c1"># this operates in-place</span>
    <span class="n">aud_epochs</span> <span class="o">=</span> <span class="n">epochs</span><span class="p">[</span><span class="s2">&quot;auditory&quot;</span><span class="p">]</span>
    <span class="n">vis_epochs</span> <span class="o">=</span> <span class="n">epochs</span><span class="p">[</span><span class="s2">&quot;visual&quot;</span><span class="p">]</span>
    <span class="k">del</span> <span class="n">raw</span><span class="p">,</span> <span class="n">epochs</span>  <span class="c1"># free up memory</span>

    <span class="c1"># %%</span>
    <span class="c1"># Like `~mne.io.Raw` objects, `~mne.Epochs` objects also have a number of</span>
    <span class="c1"># built-in plotting methods. One is `~mne.Epochs.plot_image`, which shows each</span>
    <span class="c1"># epoch as one row of an image map, with color representing signal magnitude;</span>
    <span class="c1"># the average evoked response and the sensor location are shown below the</span>
    <span class="c1"># image:</span>

    <span class="n">aud_epochs</span><span class="o">.</span><span class="n">plot_image</span><span class="p">(</span><span class="n">picks</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;MEG 1332&quot;</span><span class="p">,</span> <span class="s2">&quot;EEG 021&quot;</span><span class="p">])</span>

    <span class="c1">##############################################################################</span>
    <span class="c1"># .. note::</span>
    <span class="c1">#</span>
    <span class="c1">#     Both `~mne.io.Raw` and `~mne.Epochs` objects have `~mne.Epochs.get_data`</span>
    <span class="c1">#     methods that return the underlying data as a</span>
    <span class="c1">#     :class:`NumPy array &lt;numpy.ndarray&gt;`. Both methods have a ``picks``</span>
    <span class="c1">#     parameter for subselecting which channel(s) to return; ``raw.get_data()``</span>
    <span class="c1">#     has additional parameters for restricting the time domain. The resulting</span>
    <span class="c1">#     matrices have dimension ``(n_channels, n_times)`` for `~mne.io.Raw` and</span>
    <span class="c1">#     ``(n_epochs, n_channels, n_times)`` for `~mne.Epochs`.</span>
    <span class="c1">#</span>
    <span class="c1"># Time-frequency analysis</span>
    <span class="c1"># ^^^^^^^^^^^^^^^^^^^^^^^</span>
    <span class="c1">#</span>
    <span class="c1"># The :mod:`mne.time_frequency` submodule provides implementations of several</span>
    <span class="c1"># algorithms to compute time-frequency representations, power spectral density,</span>
    <span class="c1"># and cross-spectral density. Here, for example, we&#39;ll compute for the auditory</span>
    <span class="c1"># epochs the induced power at different frequencies and times, using Morlet</span>
    <span class="c1"># wavelets. On this dataset the result is not especially informative (it just</span>
    <span class="c1"># shows the evoked &quot;auditory N100&quot; response); see :ref:`here</span>
    <span class="c1"># &lt;inter-trial-coherence&gt;` for a more extended example on a dataset with richer</span>
    <span class="c1"># frequency content.</span>

    <span class="n">frequencies</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">power</span> <span class="o">=</span> <span class="n">mne</span><span class="o">.</span><span class="n">time_frequency</span><span class="o">.</span><span class="n">tfr_morlet</span><span class="p">(</span>
        <span class="n">aud_epochs</span><span class="p">,</span> <span class="n">n_cycles</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_itc</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">freqs</span><span class="o">=</span><span class="n">frequencies</span><span class="p">,</span> <span class="n">decim</span><span class="o">=</span><span class="mi">3</span>
    <span class="p">)</span>
    <span class="n">power</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="s2">&quot;MEG 1332&quot;</span><span class="p">])</span>

    <span class="c1"># %%</span>
    <span class="c1"># Estimating evoked responses</span>
    <span class="c1"># ^^^^^^^^^^^^^^^^^^^^^^^^^^^</span>
    <span class="c1">#</span>
    <span class="c1"># Now that we have our conditions in ``aud_epochs`` and ``vis_epochs``, we can</span>
    <span class="c1"># get an estimate of evoked responses to auditory versus visual stimuli by</span>
    <span class="c1"># averaging together the epochs in each condition. This is as simple as calling</span>
    <span class="c1"># the `~mne.Epochs.average` method on the `~mne.Epochs` object, and then using</span>
    <span class="c1"># a function from the :mod:`mne.viz` module to compare the global field power</span>
    <span class="c1"># for each sensor type of the two `~mne.Evoked` objects:</span>

    <span class="n">aud_evoked</span> <span class="o">=</span> <span class="n">aud_epochs</span><span class="o">.</span><span class="n">average</span><span class="p">()</span>
    <span class="n">vis_evoked</span> <span class="o">=</span> <span class="n">vis_epochs</span><span class="o">.</span><span class="n">average</span><span class="p">()</span>

    <span class="n">mne</span><span class="o">.</span><span class="n">viz</span><span class="o">.</span><span class="n">plot_compare_evokeds</span><span class="p">(</span>
        <span class="nb">dict</span><span class="p">(</span><span class="n">auditory</span><span class="o">=</span><span class="n">aud_evoked</span><span class="p">,</span> <span class="n">visual</span><span class="o">=</span><span class="n">vis_evoked</span><span class="p">),</span>
        <span class="n">legend</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">,</span>
        <span class="n">show_sensors</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># %%</span>
    <span class="c1"># We can also get a more detailed view of each `~mne.Evoked` object using other</span>
    <span class="c1"># plotting methods such as `~mne.Evoked.plot_joint` or</span>
    <span class="c1"># `~mne.Evoked.plot_topomap`. Here we&#39;ll examine just the EEG channels, and see</span>
    <span class="c1"># the classic auditory evoked N100-P200 pattern over dorso-frontal electrodes,</span>
    <span class="c1"># then plot scalp topographies at some additional arbitrary times:</span>

    <span class="c1"># sphinx_gallery_thumbnail_number = 13</span>
    <span class="n">aud_evoked</span><span class="o">.</span><span class="n">plot_joint</span><span class="p">(</span><span class="n">picks</span><span class="o">=</span><span class="s2">&quot;eeg&quot;</span><span class="p">)</span>
    <span class="n">aud_evoked</span><span class="o">.</span><span class="n">plot_topomap</span><span class="p">(</span><span class="n">times</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.12</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="n">ch_type</span><span class="o">=</span><span class="s2">&quot;eeg&quot;</span><span class="p">)</span>

    <span class="c1">##############################################################################</span>
    <span class="c1"># Evoked objects can also be combined to show contrasts between conditions,</span>
    <span class="c1"># using the `mne.combine_evoked` function. A simple difference can be</span>
    <span class="c1"># generated by passing ``weights=[1, -1]``. We&#39;ll then plot the difference wave</span>
    <span class="c1"># at each sensor using `~mne.Evoked.plot_topo`:</span>

    <span class="n">evoked_diff</span> <span class="o">=</span> <span class="n">mne</span><span class="o">.</span><span class="n">combine_evoked</span><span class="p">([</span><span class="n">aud_evoked</span><span class="p">,</span> <span class="n">vis_evoked</span><span class="p">],</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1"># evoked_diff.plot_topo(color=&quot;r&quot;, legend=False)</span>

    <span class="c1">##############################################################################</span>
    <span class="c1"># Inverse modeling</span>
    <span class="c1"># ^^^^^^^^^^^^^^^^</span>
    <span class="c1">#</span>
    <span class="c1"># Finally, we can estimate the origins of the evoked activity by projecting the</span>
    <span class="c1"># sensor data into this subject&#39;s :term:`source space` (a set of points either</span>
    <span class="c1"># on the cortical surface or within the cortical volume of that subject, as</span>
    <span class="c1"># estimated by structural MRI scans). MNE-Python supports lots of ways of doing</span>
    <span class="c1"># this (dynamic statistical parametric mapping, dipole fitting, beamformers,</span>
    <span class="c1"># etc.); here we&#39;ll use minimum-norm estimation (MNE) to generate a continuous</span>
    <span class="c1"># map of activation constrained to the cortical surface. MNE uses a linear</span>
    <span class="c1"># :term:`inverse operator` to project EEG+MEG sensor measurements into the</span>
    <span class="c1"># source space. The inverse operator is computed from the</span>
    <span class="c1"># :term:`forward solution` for this subject and an estimate of :ref:`the</span>
    <span class="c1"># covariance of sensor measurements &lt;tut-compute-covariance&gt;`. For this</span>
    <span class="c1"># tutorial we&#39;ll skip those computational steps and load a pre-computed inverse</span>
    <span class="c1"># operator from disk (it&#39;s included with the :ref:`sample data</span>
    <span class="c1"># &lt;sample-dataset&gt;`). Because this &quot;inverse problem&quot; is underdetermined (there</span>
    <span class="c1"># is no unique solution), here we further constrain the solution by providing a</span>
    <span class="c1"># regularization parameter specifying the relative smoothness of the current</span>
    <span class="c1"># estimates in terms of a signal-to-noise ratio (where &quot;noise&quot; here is akin to</span>
    <span class="c1"># baseline activity level across all of cortex).</span>

    <span class="c1"># load inverse operator</span>
    <span class="n">inverse_operator_file</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sample_data_folder</span> <span class="o">/</span> <span class="s2">&quot;MEG&quot;</span> <span class="o">/</span> <span class="s2">&quot;sample&quot;</span> <span class="o">/</span> <span class="s2">&quot;sample_audvis-meg-oct-6-meg-inv.fif&quot;</span>
    <span class="p">)</span>
    <span class="n">inv_operator</span> <span class="o">=</span> <span class="n">mne</span><span class="o">.</span><span class="n">minimum_norm</span><span class="o">.</span><span class="n">read_inverse_operator</span><span class="p">(</span><span class="n">inverse_operator_file</span><span class="p">)</span>
    <span class="c1"># set signal-to-noise ratio (SNR) to compute regularization parameter (λ²)</span>
    <span class="n">snr</span> <span class="o">=</span> <span class="mf">3.0</span>
    <span class="n">lambda2</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">snr</span><span class="o">**</span><span class="mi">2</span>
    <span class="c1"># generate the source time course (STC)</span>
    <span class="n">stc</span> <span class="o">=</span> <span class="n">mne</span><span class="o">.</span><span class="n">minimum_norm</span><span class="o">.</span><span class="n">apply_inverse</span><span class="p">(</span>
        <span class="n">vis_evoked</span><span class="p">,</span> <span class="n">inv_operator</span><span class="p">,</span> <span class="n">lambda2</span><span class="o">=</span><span class="n">lambda2</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;MNE&quot;</span>
    <span class="p">)</span>  <span class="c1"># or dSPM, sLORETA, eLORETA</span>

    <span class="c1">##############################################################################</span>
    <span class="c1"># Finally, in order to plot the source estimate on the subject&#39;s cortical</span>
    <span class="c1"># surface we&#39;ll also need the path to the sample subject&#39;s structural MRI files</span>
    <span class="c1"># (the ``subjects_dir``):</span>

    <span class="c1"># path to subjects&#39; MRI files</span>
    <span class="n">subjects_dir</span> <span class="o">=</span> <span class="n">sample_data_folder</span> <span class="o">/</span> <span class="s2">&quot;subjects&quot;</span>
    <span class="c1"># plot the STC</span>
    <span class="n">stc</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="n">initial_time</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">hemi</span><span class="o">=</span><span class="s2">&quot;split&quot;</span><span class="p">,</span> <span class="n">views</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;lat&quot;</span><span class="p">,</span> <span class="s2">&quot;med&quot;</span><span class="p">],</span> <span class="n">subjects_dir</span><span class="o">=</span><span class="n">subjects_dir</span>
    <span class="p">)</span></div>


    <span class="c1">##############################################################################</span>
    <span class="c1"># The remaining tutorials have *much more detail* on each of these topics (as</span>
    <span class="c1"># well as many other capabilities of MNE-Python not mentioned here:</span>
    <span class="c1"># connectivity analysis, encoding/decoding models, lots more visualization</span>
    <span class="c1"># options, etc). Read on to learn more!</span>
    <span class="c1">#</span>
    <span class="c1"># .. LINKS</span>
    <span class="c1">#</span>
    <span class="c1"># .. _`autoreject package`: http://autoreject.github.io/</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main&#39;</span><span class="p">:</span>
    <span class="n">MNE_tutorial_EEG_MEG_processing</span><span class="p">()</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Nicolas Eby.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>